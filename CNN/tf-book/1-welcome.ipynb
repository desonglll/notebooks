{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T02:18:24.371038Z",
     "start_time": "2024-12-24T02:18:22.016591Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T02:18:25.399776Z",
     "start_time": "2024-12-24T02:18:25.304078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = \"\"\"Dear Amazon, last week I ordered an Optimus Prime action figure \\\n",
    "from your online store in Germany. Unfortunately, when I opened the package, \\\n",
    "I discovered to my horror that I had been sent an action figure of Megatron \\\n",
    "instead! As a lifelong enemy of the Decepticons, I hope you can understand my \\\n",
    "dilemma. To resolve the issue, I demand an exchange of Megatron for the \\\n",
    "Optimus Prime figure I ordered. Enclosed are copies of my records concerning \\\n",
    "this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\"\"\"\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ],
   "id": "ce515cf05bbeba59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.901546"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.901546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T02:18:30.490243Z",
     "start_time": "2024-12-24T02:18:26.417788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ],
   "id": "506fd32ecb54dd94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  entity_group     score           word  start  end\n",
       "0          ORG  0.879009         Amazon      5   11\n",
       "1         MISC  0.990859  Optimus Prime     36   49\n",
       "2          LOC  0.999755        Germany     90   97\n",
       "3         MISC  0.556569           Mega    208  212\n",
       "4          PER  0.590257         ##tron    212  216\n",
       "5          ORG  0.669692         Decept    253  259\n",
       "6         MISC  0.498349        ##icons    259  264\n",
       "7         MISC  0.775361       Megatron    350  358\n",
       "8         MISC  0.987854  Optimus Prime    367  380\n",
       "9          PER  0.812096      Bumblebee    502  511"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.990859</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>Germany</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.556569</td>\n",
       "      <td>Mega</td>\n",
       "      <td>208</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.590257</td>\n",
       "      <td>##tron</td>\n",
       "      <td>212</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.669692</td>\n",
       "      <td>Decept</td>\n",
       "      <td>253</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>##icons</td>\n",
       "      <td>259</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.775361</td>\n",
       "      <td>Megatron</td>\n",
       "      <td>350</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.987854</td>\n",
       "      <td>Optimus Prime</td>\n",
       "      <td>367</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.812096</td>\n",
       "      <td>Bumblebee</td>\n",
       "      <td>502</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T02:31:51.636618Z",
     "start_time": "2024-12-24T02:31:48.995393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ],
   "id": "f1f24e517dd684f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      score  start  end                   answer\n",
       "0  0.631292    335  358  an exchange of Megatron"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631292</td>\n",
       "      <td>335</td>\n",
       "      <td>358</td>\n",
       "      <td>an exchange of Megatron</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T02:32:23.570527Z",
     "start_time": "2024-12-24T02:32:20.642505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=100, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ],
   "id": "1ed155f9e758a709",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m summarizer \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msummarization\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43msummarizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msummary_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:274\u001B[0m, in \u001B[0;36mSummarizationPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    251\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;124;03m    Summarize the text(s) given as inputs.\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;124;03m          ids of the summary.\u001B[39;00m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:167\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    139\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;124;03m          ids of the generated text.\u001B[39;00m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 167\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    169\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m)\n\u001B[1;32m    170\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(el, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m el \u001B[38;5;129;01min\u001B[39;00m args[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mlen\u001B[39m(res) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m result)\n\u001B[1;32m    172\u001B[0m     ):\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [res[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m result]\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1301\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1294\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1295\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1298\u001B[0m         )\n\u001B[1;32m   1299\u001B[0m     )\n\u001B[1;32m   1300\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1308\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1307\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1308\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1309\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/base.py:1208\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m   1207\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m-> 1208\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1209\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m   1210\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:196\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline._forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[1;32m    194\u001B[0m     generate_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgeneration_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneration_config\n\u001B[0;32m--> 196\u001B[0m output_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m out_b \u001B[38;5;241m=\u001B[39m output_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2030\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   2027\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   2029\u001B[0m device \u001B[38;5;241m=\u001B[39m inputs_tensor\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m-> 2030\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_special_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs_has_attention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2032\u001B[0m \u001B[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001B[39;00m\n\u001B[1;32m   2033\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m   2034\u001B[0m     \u001B[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001B[39;00m\n\u001B[1;32m   2035\u001B[0m     \u001B[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1879\u001B[0m, in \u001B[0;36mGenerationMixin._prepare_special_tokens\u001B[0;34m(self, generation_config, kwargs_has_attention_mask, device)\u001B[0m\n\u001B[1;32m   1873\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1874\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1875\u001B[0m     )\n\u001B[1;32m   1876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():  \u001B[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001B[39;00m\n\u001B[1;32m   1877\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1878\u001B[0m         eos_token_tensor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1879\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[43misin_mps_friendly\u001B[49m\u001B[43m(\u001B[49m\u001B[43melements\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_elements\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_tensor\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39many()\n\u001B[1;32m   1880\u001B[0m     ):\n\u001B[1;32m   1881\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs_has_attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs_has_attention_mask:\n\u001B[1;32m   1882\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[1;32m   1883\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1884\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1885\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`attention_mask` to obtain reliable results.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1886\u001B[0m             )\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pytorch_utils.py:341\u001B[0m, in \u001B[0;36misin_mps_friendly\u001B[0;34m(elements, test_elements)\u001B[0m\n\u001B[1;32m    338\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m elements\u001B[38;5;241m.\u001B[39mtile(test_elements\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39meq(test_elements\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msum(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mbool()\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misin\u001B[49m\u001B[43m(\u001B[49m\u001B[43melements\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_elements\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T02:38:07.236158Z",
     "start_time": "2024-12-24T02:35:42.162425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator = pipeline(\"translation_en_to_de\",\n",
    "                      model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ],
   "id": "1f3ccac69e35d054",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "964b603e86dd4251b1de4f62cf529775"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f4c88791a9b4bf1ac8fc621e8a60c56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4415d972087a4b10879506eb6aa77e9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c0154767c654cf9949ff83acc4b4694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m translator \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtranslation_en_to_de\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHelsinki-NLP/opus-mt-en-de\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m outputs \u001B[38;5;241m=\u001B[39m translator(text, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, min_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtranslation_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1047\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m   1044\u001B[0m             tokenizer_kwargs \u001B[38;5;241m=\u001B[39m model_kwargs\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m   1045\u001B[0m             tokenizer_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1047\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1048\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtokenizer_identifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtokenizer_kwargs\u001B[49m\n\u001B[1;32m   1049\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_image_processor:\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001B[39;00m\n\u001B[1;32m   1053\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m image_processor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/cnn-Pz-mZeQz-py3.10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:945\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    943\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class_py\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    944\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 945\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    946\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    947\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min order to use this tokenizer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    948\u001B[0m             )\n\u001B[1;32m    950\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    951\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to build an AutoTokenizer.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    952\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mTOKENIZER_MAPPING\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    953\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ],
   "id": "203dd0eeaa1bc5bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
